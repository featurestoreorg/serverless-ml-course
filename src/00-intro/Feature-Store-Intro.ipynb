{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "076a7f63",
   "metadata": {},
   "source": [
    "## Feature Store Basics\n",
    "\n",
    "This notebook introduces the basics of working with the Hopsworks API and Pandas DataFrames.\n",
    "\n",
    "First, we will define a Pandas DataFrame with 4 credit card transactions in 3 different cities with the same credit card. The last 2 credit card transactions are labeled as 'fraud', while the first 2 transactions are labeled as 'not fraud'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9411b6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>credit_card_number</th>\n",
       "      <th>trans_datetime</th>\n",
       "      <th>amount</th>\n",
       "      <th>location</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1111 2222 3333 4444</td>\n",
       "      <td>2022-01-01 08:44:00</td>\n",
       "      <td>142.34</td>\n",
       "      <td>Sao Paolo</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1111 2222 3333 4444</td>\n",
       "      <td>2022-01-02 19:44:00</td>\n",
       "      <td>12.34</td>\n",
       "      <td>Rio De Janeiro</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1111 2222 3333 4444</td>\n",
       "      <td>2022-01-02 20:44:00</td>\n",
       "      <td>66.29</td>\n",
       "      <td>Stockholm</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1111 2222 3333 4444</td>\n",
       "      <td>2022-01-02 20:55:00</td>\n",
       "      <td>112.33</td>\n",
       "      <td>Stockholm</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    credit_card_number      trans_datetime  amount        location  fraud\n",
       "0  1111 2222 3333 4444 2022-01-01 08:44:00  142.34       Sao Paolo  False\n",
       "1  1111 2222 3333 4444 2022-01-02 19:44:00   12.34  Rio De Janeiro  False\n",
       "2  1111 2222 3333 4444 2022-01-02 20:44:00   66.29       Stockholm   True\n",
       "3  1111 2222 3333 4444 2022-01-02 20:55:00  112.33       Stockholm   True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = { \n",
    "    'credit_card_number': ['1111 2222 3333 4444', '1111 2222 3333 4444','1111 2222 3333 4444',\n",
    "                           '1111 2222 3333 4444'],\n",
    "    'trans_datetime': ['2022-01-01 08:44', '2022-01-02 19:44', '2022-01-02 20:44', '2022-01-02 20:55'],\n",
    "    'amount': [142.34, 12.34, 66.29, 112.33],\n",
    "    'location': ['Sao Paolo', 'Rio De Janeiro', 'Stockholm', 'Stockholm'],\n",
    "    'fraud': [False, False, True, True] \n",
    "}\n",
    "\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "df['trans_datetime']= pd.to_datetime(df['trans_datetime'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304e9f2d",
   "metadata": {},
   "source": [
    "## Connect to Hopsworks\n",
    "\n",
    "You need an API key to connect. First, login to Hopsworks, then run this code. It will provide a link to get your API key, that you then need to copy and paste into the text box that appears below this cell.\n",
    "\n",
    "It is good practice to save this API key somewhere safe so you don't have to create a new one every time you use Hopsworks. If you run this code on your laptop, a copy of the API key will be cached locally in this directory in a file with restricted permissions, so you don't have to always re-enter the API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c855e3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/398\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "proj = hopsworks.login()\n",
    "fs = proj.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d38175",
   "metadata": {},
   "source": [
    "### Create a Feature Group\n",
    "\n",
    "A feature group is a table of features that are computed together in the same feature pipeline and written as a DataFrame to the Feature Store. You should have a unique idenitfier for each row that may be one or more columns, and you define as the `primary_key`. You may also have a column that represents the timestamp or datetime for when row values were observed. If so, you should specify the `event_time` column when creating the Feature Group.\n",
    "\n",
    "Hopsworks have comprehensive documentation on Feature Groups. Click on these links to learn more.\n",
    "\n",
    "* [Feature Group Concept](https://docs.hopsworks.ai/3.0/concepts/fs/feature_group/fg_overview/)\n",
    "* [Feature Group Creation Guide](https://docs.hopsworks.ai/3.0/user_guides/fs/feature_group/create/)\n",
    "* [Feature Group API Docs](https://docs.hopsworks.ai/feature-store-api/3.0/generated/api/feature_group_api/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ea7a88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg = fs.get_or_create_feature_group(\n",
    "     name=\"credit_card_transactions\",\n",
    "     version=1,\n",
    "     description=\"Credit Card Transaction data\",\n",
    "     primary_key=['credit_card_number'],\n",
    "     event_time='trans_datetime'\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990c0972",
   "metadata": {},
   "source": [
    "### Write your DataFrame to the Feature Group\n",
    "When you write your DataFrame to the feature group, first the DataFrame is copied to Hopsworks. \n",
    "Then a backfill ingestion job is run on Hopsworks to insert/append the DataFrame to the Feature Group. \n",
    "The job is a Spark job, and the data is stored in a Apache Hudi table in Hopsworks.\n",
    "\n",
    "It will take about 1 minute for the ingestion job to complete.\n",
    "If you don't want to wait 1 minute, you make the ingestion job run in the background with:\n",
    "\n",
    "\n",
    "    fg.insert(df, write_options={\"wait_for_job\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3380610c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c290a8882f4dbba18d2598b478aea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading Dataframe: 0.00% |          | Rows 0/4 | Elapsed Time: 00:00 | Remaining Time: ?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching offline feature group backfill job...\n",
      "Backfill Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai/p/398/jobs/named/credit_card_transactions_1_offline_fg_backfill/executions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<hsfs.core.job.Job at 0x7f751430ee80>, None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fg.insert(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef2bc45",
   "metadata": {},
   "source": [
    "## Read using Feature Views\n",
    "\n",
    "When you want to use features to train or serve models, you create Feature that are `labels` View a Feature View by first selecting features from Feature Groups. Here, we only have 1 Feature Group, and we select 3 features from it, returning a `query` object. The `query` object defines the set of features (or schema) for a Feature View. \n",
    "\n",
    "You create a Feature View with a `query` object (specifying the features and any extra columns that might be needed for inference (but not training)), providing a name and version, and specifying the columns that are `labels`, that is, the target your machine learning algorithm will try and optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac7e3a12",
   "metadata": {},
   "outputs": [
    {
     "ename": "RestAPIError",
     "evalue": "Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/project/398/featurestores/335/featureview). Server response: \nHTTP code: 400, HTTP reason: Bad Request, error code: 270179, error msg: The provided feature view name and version already exists, user msg: Feature view: credit_card_transactions, version: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRestAPIError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3638955/1751349647.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"amount\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"location\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fraud\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m fv = fs.create_feature_view(name=\"credit_card_transactions\",\n\u001b[0m\u001b[1;32m      4\u001b[0m                             \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Features from the credit_card_transactions FG\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/hsfs/feature_store.py\u001b[0m in \u001b[0;36mcreate_feature_view\u001b[0;34m(self, name, query, version, description, labels, transformation_functions)\u001b[0m\n\u001b[1;32m    947\u001b[0m             \u001b[0mtransformation_functions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransformation_functions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m         )\n\u001b[0;32m--> 949\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feature_view_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat_view\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_feature_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/hsfs/core/feature_view_engine.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, feature_view_obj)\u001b[0m\n\u001b[1;32m     67\u001b[0m             ]\n\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_function_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattach_transformation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_view_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mupdated_fv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feature_view_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_view_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         print(\n\u001b[1;32m     71\u001b[0m             \u001b[0;34m\"Feature view created successfully, explore it at \\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/hsfs/core/feature_view_api.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, feature_view_obj)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"content-type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"application/json\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         return feature_view_obj.update_from_response_json(\n\u001b[0;32m---> 54\u001b[0;31m             self._client._send_request(\n\u001b[0m\u001b[1;32m     55\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_POST\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/hsfs/decorators.py\u001b[0m in \u001b[0;36mif_connected\u001b[0;34m(inst, *args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connected\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNoHopsworksConnectionError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mif_connected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/hsfs/client/base.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, path_params, query_params, headers, data, stream, files)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRestAPIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRestAPIError\u001b[0m: Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/project/398/featurestores/335/featureview). Server response: \nHTTP code: 400, HTTP reason: Bad Request, error code: 270179, error msg: The provided feature view name and version already exists, user msg: Feature view: credit_card_transactions, version: 1"
     ]
    }
   ],
   "source": [
    "query = fg.select([\"amount\", \"location\", \"fraud\"])\n",
    "\n",
    "fv = fs.create_feature_view(name=\"credit_card_transactions\",\n",
    "                            version=1,\n",
    "                            description=\"Features from the credit_card_transactions FG\",\n",
    "                            labels=[\"fraud\"],\n",
    "                            query=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548b2099",
   "metadata": {},
   "source": [
    "### Splitting into Train/Test sets\n",
    "\n",
    "With a Feature View, you can read train and test sets directly as Pandas DataFrames - similar to scikit-learn.\n",
    "Here, \n",
    "\n",
    "* `X_train` is the features of our train set, \n",
    "* `y_train` is the labels of our train set, \n",
    "* `X_test` is the features of our test set, \n",
    "* `y_test` is the labels of our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792da473",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = fv.train_test_split(0.5)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fef54b2",
   "metadata": {},
   "source": [
    "### Saving training data as files\n",
    "Sometimes, if you have a large volume of training data, it is better to save training data as files. Then read the files in your training pipeline. You can create training data as CSV files that is randomly split into train/test sets as follows (the `td_version` is the version of the training data for this feature view, and you can track the progress of the job used to create the training data using the `td_job` object)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578e424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "td_version, td_job = fv.create_train_test_split(\n",
    "    description = 'Transactions fraud batch training dataset',\n",
    "    data_format = 'csv',\n",
    "    test_size = 0.5,\n",
    "    write_options = {'wait_for_job': True},\n",
    "    coalesce = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186b958f",
   "metadata": {},
   "source": [
    "## Training Data as files\n",
    "The training data is now stored as a CSV file on Hopsworks under `Project Settings` -> `File Browser` -> <username>_Training_Datasets.\n",
    "    \n",
    "You can read the training data as split train/test sets with the following. Note the parameter `td_version` we pass here. A feature view can have many training datasets, so you need to supply the version you want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e53364",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = fv.get_train_test_split(td_version)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f6d416",
   "metadata": {},
   "source": [
    "### Aggregations\n",
    "\n",
    "Compute the total amount spent on the credit card by first grouping all the rows together with the same `credit_card_number` and then summing up their amounts. \n",
    "\n",
    "The code first creates a new DataFrame with only the `credit_card_number` and `amount` columns, then the logic of a group-by could be described as \n",
    "\n",
    "    for-each (`credit_card_number`) do \\sigma amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4be1e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[[\"credit_card_number\", \"amount\"]].groupby(\"credit_card_number\").sum()\n",
    "df2.rename(columns={\"amount\": \"total_spent\"}, inplace=True)\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0be838",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d187468e",
   "metadata": {},
   "source": [
    " We might also want to know at what point-in-time was that total and add a column with the datetime of the last (most recent) credit card transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04244e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"as_of_datetime\"] = df[[\"credit_card_number\", \"trans_datetime\"]].groupby(\"credit_card_number\").max()\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad27a872",
   "metadata": {},
   "source": [
    "The `groupby` operation sets `credit_card_number` as the index of our DataFrame.\n",
    "We want `credit_card_number` as a column, as Pandas indexes are not written to the Feature Group.\n",
    "We can move the index to a column using `reset_index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec0104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.reset_index(inplace=True)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8253595",
   "metadata": {},
   "source": [
    "We create a feature group to store the contents of `df2` with our aggregated credit card spending information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c321ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg2 = fs.get_or_create_feature_group(\n",
    "     name=\"credit_card_spending\",\n",
    "     version=1,\n",
    "     description=\"Credit Card Spending\",\n",
    "     primary_key=['credit_card_number'],\n",
    "     event_time='as_of_datetime'\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712f283f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg2.insert(df2, write_options={\"wait_for_job\": False})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b7aedf",
   "metadata": {},
   "source": [
    "Let's add some more data to our original feature group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac3a4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "more_data = { \n",
    "    'credit_card_number': ['9999 8888 7777 6666', '9999 8888 7777 6666','9999 8888 7777 6666',\n",
    "                           '9999 8888 7777 6666'],\n",
    "    'trans_datetime': ['2022-01-02 04:11', '2022-01-03 07:24', '2022-01-05 10:33', '2022-01-05 11:50'],\n",
    "    'amount': [55.67, 84, 77.95, 183],\n",
    "    'location': ['San Francisco', 'San Francisco', 'Dublin', 'Dublin'],\n",
    "    'fraud': [False, False, False, False] \n",
    "}\n",
    "\n",
    "df3 = pd.DataFrame.from_dict(more_data)\n",
    "df3['trans_datetime']= pd.to_datetime(df3['trans_datetime'])\n",
    "\n",
    "fg = fs.get_feature_group(name=\"credit_card_transactions\", version=1)\n",
    "\n",
    "fg.insert(df3, write_options={\"wait_for_job\": False})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a1d883",
   "metadata": {},
   "source": [
    "Now let's compute how much money was spent on the card since the last time we computed amount spent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a2dc55",
   "metadata": {},
   "source": [
    "## Time Series: Window Aggregations\n",
    "\n",
    "Count the amount of money spent per day (make the length of the window '1d').\n",
    "We will need to set the `event_time` column as the index in order to use Pandas built-in window aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2baba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = fg.read()\n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864dc8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df5.set_index('trans_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b325eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df5.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51e0b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "    df5['rolling_max_1d'] = df5.rolling('1D').amount.max()\n",
    "    df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dec426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5['rolling_mean_1d'] = df5.rolling('1D').amount.mean()\n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683a0337",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114b74d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_agg = fs.get_or_create_feature_group(\n",
    "     name=\"credit_card_rolling_windows\",\n",
    "     version=1,\n",
    "     description=\"Daily Credit Card Spending\",\n",
    "     primary_key=['credit_card_number'],\n",
    "     event_time='trans_datetime'\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b05b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_agg.insert(df5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31174a83",
   "metadata": {},
   "source": [
    "### Create a Feature View using features from multiple Feature Groups\n",
    "\n",
    "We want to create a model that uses features from multiple feature groups. \n",
    "We will select features from the different feature groups and join them together to create a query object. \n",
    "We can read the data in the query object as a DataFrame to inspect it before we create the feature view. \n",
    "We will use the feature view to read the training data for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf25f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = fg.select_all().join(fg_agg.select(['rolling_max_1d', 'rolling_mean_1d']))\n",
    "\n",
    "training_data = query.read()\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa4cbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fv = fs.create_feature_view(name=\"credit_card_fraud_rolling\",\n",
    "                            description=\"Features for a model to predict credit card fraud, including rolling windows\",\n",
    "                            version=1,\n",
    "                            query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a6e33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = fv.train_test_split(0.5)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7576002",
   "metadata": {},
   "source": [
    "### Read from Feature Groups\n",
    "\n",
    "You are also able to read data from Feature Groups as DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1717fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg = fs.get_feature_group(name=\"credit_card_transactions\", version=1)\n",
    "read_df = fg.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67029b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5aa606",
   "metadata": {},
   "source": [
    "### Filters\n",
    "You can use filters on the `query` object or on the Feature Groups, when reading from them. Here, we read all rows where the transaction amount is greater than 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4d9483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsfs.feature import Feature\n",
    "\n",
    "big_amounts_df = fg.filter(Feature(\"amount\") > 100).read()\n",
    "big_amounts_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
